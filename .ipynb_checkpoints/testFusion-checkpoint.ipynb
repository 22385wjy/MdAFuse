{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import imageio\n",
    "import natsort\n",
    "from pytorch_msssim import ssim\n",
    "import os\n",
    "import os.path\n",
    "import multiprocessing\n",
    "import scipy.io as scio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib\n",
    "import scipy.misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "device ='cpu'\n",
    "\n",
    "print(device)\n",
    "\n",
    "image_length = 256\n",
    "image_width = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LFConv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_LFConv1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class _LFConv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_LFConv2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=2, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2, 4, 7, 1, 3),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(4, 8, 7, 1, 3),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 7, 1, 3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 7, 1, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class _HFConv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_HFConv1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class _HFConv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_HFConv2, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #self.up = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def upsample(self, x, size):\n",
    "        return nn.functional.interpolate(x, size, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.pool(x1)\n",
    "        x3=self.conv2(x2)\n",
    "        x4=self.conv3(x3)\n",
    "        x5=self.upsample(x4,(256,256))\n",
    "        x1_2=self.conv2(x1)\n",
    "        x5_2=self.conv6(x5)\n",
    "        cat=torch.cat((x1_2,x5_2),1)\n",
    "        x6=self.conv4(cat)\n",
    "\n",
    "        return x6\n",
    "\n",
    "def denorm(mean=[0, 0, 0], std=[1, 1, 1], tensor=None):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "class forOriginal(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n",
    "        super(forOriginal, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DsConv(nn.Module):\n",
    "    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n",
    "        super(DsConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),\n",
    "            nn.BatchNorm2d(dw_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dw_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class downsample(nn.Module):\n",
    "    def __init__(self, dw_channels1=16, dw_channels2=32, out_channels=64, **kwargs):\n",
    "        super(downsample, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, dw_channels1, 3, 2),\n",
    "            nn.BatchNorm2d(dw_channels1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dsconv1 = DsConv(dw_channels1, dw_channels2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.dsconv1(x1)\n",
    "        \n",
    "        return x1,x2\n",
    "\n",
    "class SegConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n",
    "        super(SegConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class SegConv2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, **kwargs):\n",
    "        super(SegConv2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, 1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class PoolandUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(PoolandUp, self).__init__()\n",
    "        inter_channels = int(in_channels / 2)\n",
    "        self.conv1 = SegConv(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.out = SegConv(in_channels, out_channels, 1)\n",
    "\n",
    "    def pool(self, x, size):\n",
    "        avgpool = nn.AdaptiveAvgPool2d(size)\n",
    "        return avgpool(x)\n",
    "\n",
    "    def upsample(self, x, size):\n",
    "        return nn.functional.interpolate(x, size, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        p1 = self.pool(x, int(size[0]/2))\n",
    "        cp1 = self.conv1(p1)\n",
    "        ucp1 = self.upsample(cp1, size)\n",
    "\n",
    "        x=self.upsample(self.conv1(x), size)\n",
    "\n",
    "        x = torch.cat([x, ucp1], dim=1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SeFeature(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t=2, stride=2, **kwargs):\n",
    "        super(SeFeature, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            SegConv(in_channels, in_channels * t, 1),\n",
    "            SegConv2(in_channels * t, in_channels * t ),\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.pool_up=PoolandUp(32,32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.block(x)\n",
    "        out=self.pool_up(out1)\n",
    "        return out\n",
    "\n",
    "class segment(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, stride=1, **kwargs):\n",
    "        super(segment, self).__init__()\n",
    "        self.conv1 = DsConv(in_channels, in_channels, stride)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(in_channels, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv(x1)\n",
    "\n",
    "        return x2\n",
    "\n",
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.alpha=nn.Parameter(torch.ones(1,1,num_features))\n",
    "        self.beta=nn.Parameter(torch.zeros(1,1,num_features))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.alpha*x+self.beta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    # define the network\n",
    "    class DeepTreeFuse(nn.Module):\n",
    "        def __init__(self,num_classes=3):\n",
    "            super(DeepTreeFuse, self).__init__()\n",
    "\n",
    "            #####one lf layer 1#####\n",
    "            self.one_lf1 =_LFConv1()\n",
    "            self.one_lf2 = _LFConv2()\n",
    "            #####one hf layers#####\n",
    "            self.one_hf1 =_HFConv1()\n",
    "            self.one_hf2 = _HFConv2()\n",
    "\n",
    "            #####two lf layer 1#####\n",
    "            self.two_lf1 = _LFConv1()\n",
    "            self.two_lf2 = _LFConv2()\n",
    "            #####two hf layers#####\n",
    "            self.two_hf1 = _HFConv1()\n",
    "            self.two_hf2 = _HFConv2()\n",
    "\n",
    "            self.l_o=forOriginal(1,16,7,1,3)\n",
    "            self.h_o = forOriginal(1, 16, 3, 1, 3)\n",
    "\n",
    "            # lf reconstruction\n",
    "            self.lf_recons = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64, out_channels=32, kernel_size=5, stride=1,\n",
    "                          padding=2),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(in_channels=32, out_channels=16, kernel_size=5, stride=1,\n",
    "                          padding=2),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True)\n",
    "            )  # output shape (,16,256,256)\n",
    "\n",
    "            # hf reconstruction\n",
    "            self.hf_recons = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=256, out_channels=128, kernel_size=5, stride=1,\n",
    "                          padding=2),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(in_channels=128, out_channels=64, kernel_size=5, stride=1,\n",
    "                          padding=2),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(in_channels=64, out_channels=32, kernel_size=5, stride=1,\n",
    "                          padding=2),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(in_channels=32, out_channels=16, kernel_size=5, stride=1,\n",
    "                          padding=2),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True),\n",
    "            )  # output shape (,16,256,256)\n",
    "\n",
    "            # final reconstruction\n",
    "            self.recon = nn.Sequential(  # input shape (,16, 256, 256)\n",
    "                nn.Conv2d(in_channels=16, out_channels=3, kernel_size=5, stride=1,\n",
    "                          padding=2))  # output shape (,3,256,256)\n",
    "            self.hf2con = nn.Sequential(  # input shape (,16, 256, 256)\n",
    "                nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, stride=1,\n",
    "                          padding=2))\n",
    "            self.reconse = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=1,\n",
    "                          padding=2))\n",
    "            self.re = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1,\n",
    "                          padding=1))\n",
    "\n",
    "            # semantic feature extract\n",
    "            self.downsample = downsample()\n",
    "            self.globalfeature = SeFeature(32, 32)\n",
    "            self.otherconv = SegConv2(32, 32)\n",
    "            self.merge = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1,\n",
    "                          padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "            self.segclass = segment(32, num_classes=3)\n",
    "            self.dropandplt = nn.Sequential(\n",
    "                nn.Conv2d(32, 16, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Conv2d(16, num_classes, 1)\n",
    "            )\n",
    "            self.x0 = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1, bias=False),\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.ReLU(True))\n",
    "            self.x1 = nn.Sequential(nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.ReLU(True))\n",
    "            self.x2 = nn.Sequential(nn.Conv2d(32, 32, 3, padding=1, bias=False),\n",
    "                                    nn.BatchNorm2d(32),\n",
    "                                    nn.ReLU(True))\n",
    "            self.xxx = nn.Sequential(nn.Conv2d(96, 64, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(64),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Conv2d(64, 32, 3, padding=1, bias=False),\n",
    "                                     nn.BatchNorm2d(32),\n",
    "                                     nn.ReLU(True)\n",
    "                                     )\n",
    "            self.aff=AffineTransform(256)\n",
    "\n",
    "        def upsample(self, x, size):\n",
    "            return nn.functional.interpolate(x, size, mode='bilinear', align_corners=True)\n",
    "\n",
    "        def tensor_max(self, tensors):\n",
    "            max_tensor = None\n",
    "            for i, tensor in enumerate(tensors):\n",
    "                if i == 0:\n",
    "                    max_tensor = tensor\n",
    "                else:\n",
    "                    max_tensor = torch.max(max_tensor, tensor)\n",
    "            return max_tensor\n",
    "\n",
    "        def obtain_to_LHF(self, tensor):\n",
    "            # lf\n",
    "            x=tensor\n",
    "            lf1 = self.one_lf1(x)\n",
    "            lf2 = self.one_lf2(x)\n",
    "            x1 = torch.cat((lf1, lf2), 1)\n",
    "            lf_x = self.lf_recons(x1)\n",
    "            # hf\n",
    "            hf1 = self.one_hf1(x)\n",
    "            hf2 = self.one_hf2(x)\n",
    "            x2 = torch.cat((hf1, hf2), 1)\n",
    "            hf_x = self.hf_recons(x2)\n",
    "            x_change_c_ol = self.l_o(x)\n",
    "            x_change_c_oh = self.h_o(x)\n",
    "            outx =lf_x+hf_x\n",
    "\n",
    "            return lf_x,hf_x,outx,x_change_c_ol,x_change_c_oh\n",
    "\n",
    "        def semantic_one(self,tensor):\n",
    "            x=tensor\n",
    "            sx, se = self.downsample(x)\n",
    "            s1 = self.globalfeature(se)\n",
    "            s2 = self.otherconv(se)\n",
    "            s = s1 + s2\n",
    "            out_s = self.merge(s)\n",
    "            out_up = self.upsample(out_s, (128, 128))\n",
    "            sclass = self.segclass(out_up)\n",
    "            size = x.size()[2:]\n",
    "            sclass = self.upsample(sclass, size)\n",
    "            result_S = sclass\n",
    "            x0 = self.x0(x)\n",
    "            sx = self.x1(sx)\n",
    "            se2 = self.x2(se)\n",
    "            sx = self.upsample(sx, size)\n",
    "            se2 = self.upsample(se2, size)\n",
    "            x0 = self.dropandplt(x0)  # self.segclass(x0)\n",
    "            sx = self.segclass(sx)\n",
    "            se2 = self.segclass(se2)\n",
    "            x3_out = x0 + sx + se2\n",
    "            total_seg1 = result_S + x3_out\n",
    "            \n",
    "            return total_seg1\n",
    "    \n",
    "        def doFuse(self,t1,t2,t3,t4,xl,xh,yl,yh,cx_l,cx_h,cy_l,cy_h):\n",
    "            lhf_x=t1\n",
    "            lhf_y=t2\n",
    "            se_x=t3\n",
    "            se_y=t4\n",
    "            xh=self.aff(xh)\n",
    "            xl=self.aff(xl)\n",
    "            yh=self.aff(yh)\n",
    "            yl=self.aff(yl)\n",
    "            \n",
    "            HF = xh + yh\n",
    "            LF = xl + yl\n",
    "            fu_lf_hf = self.tensor_max([HF, LF])\n",
    "\n",
    "            lf_hf = self.recon(fu_lf_hf)\n",
    "            lf_hf = self.aff(lf_hf)\n",
    "\n",
    "            out_se = (se_x + se_y) / 2\n",
    "            out_se = self.aff(out_se)\n",
    "            \n",
    "            fuseout = (lf_hf + out_se) / 2\n",
    "            fuseout = torch.tanh(fuseout)\n",
    "\n",
    "            return fuseout\n",
    "\n",
    "        def forward(self, x, y): # lf,hf,pyramid and reconstruction of mul-type\n",
    "            #print(torch.equal(x, y))\n",
    "            ## extract deep features\n",
    "            # __________________ one type __________________\n",
    "\n",
    "            lf_x,hf_x,lhf_x,cx_l,cx_h=self.obtain_to_LHF(x)\n",
    "            se_x=self.semantic_one(x)  # semantic\n",
    "            #print('one type result out **********')\n",
    "\n",
    "            # __________________ two type __________________\n",
    "            lf_y, hf_y, lhf_y,cy_l,cy_h = self.obtain_to_LHF(y)\n",
    "            se_y = self.semantic_one(y)\n",
    "            #print('the other type result out **********')\n",
    "            \n",
    "            ## to do fusing\n",
    "            fuseout=self.doFuse(lhf_x,lhf_y,se_x,se_y,lf_x,hf_x,lf_y, hf_y,cx_l,cx_h,cy_l,cy_h)\n",
    "            fuseout=self.re(fuseout)\n",
    "            \n",
    "            return fuseout\n",
    "\n",
    "    dtn = DeepTreeFuse().to(device)\n",
    "    dtn = dtn.float()\n",
    "    #print(dtn)\n",
    "\n",
    "    return dtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mri_to_do_test():\n",
    "    # load the test input MRI dataset\n",
    "    dataset = os.path.join(os.getcwd(), './data_to_testFusing/MRT2')\n",
    "    data = glob.glob(os.path.join(dataset, \"*.*\"))\n",
    "    \n",
    "    data = natsort.natsorted(data, reverse=False)\n",
    "    test_mri = np.zeros((len(data), image_width, image_length))\n",
    "    for i in range(len(data)):\n",
    "        test_mri[i, :, :] = (imageio.imread(data[i]))\n",
    "        test_mri[i, :, :] = (test_mri[i, :, :] - np.min(test_mri[i, :, :])) / (\n",
    "                    np.max(test_mri[i, :, :]) - np.min(test_mri[i, :, :]))\n",
    "        test_mri[i, :, :] = np.float32(test_mri[i, :, :])\n",
    "\n",
    "    # expand dimension to add the channel\n",
    "    test_mri = np.expand_dims(test_mri, axis=1)\n",
    "    # verify the shape matches the pytorch standard\n",
    "    print(test_mri.shape)\n",
    "\n",
    "    #plt.imshow(test_mri[0, 0, :, :], 'gray')\n",
    "    # plt.savefig('MRI.png', bbox_inches = 'tight',pad_inches = 0,dpi=200)\n",
    "\n",
    "    # convert the MRI Testing data to pytorch tensor\n",
    "    test_mri_tensor = torch.from_numpy(test_mri).float()\n",
    "    test_mri_tensor = test_mri_tensor.to(device)\n",
    "    print(test_mri_tensor.shape)\n",
    "    test_mri_tensor.requires_grad = True\n",
    "\n",
    "    return test_mri_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pet_to_do_test():\n",
    "    dataset = os.path.join(os.getcwd(), './data_to_testFusing/SPECTTc/')  \n",
    "    data = glob.glob(os.path.join(dataset, \"*.*\"))\n",
    "    data = natsort.natsorted(data, reverse=False)\n",
    "    pet_channels=4\n",
    "    train_other = np.zeros((len(data), image_width, image_length, pet_channels), dtype=float)\n",
    "    test_pet = np.zeros((len(data), image_width, image_length), dtype=float)\n",
    "    #train_one = np.zeros((len(data), image_width, image_length,3), dtype=float) #(272,256,256,3)\n",
    "    #data_2 = np.zeros((len(data), image_width, image_length, 3))\n",
    "    for i in range(len(data)):\n",
    "        # train_pet[i, :, :] = (imageio.imread(data[i]))\n",
    "        train_other[i, :, :, :] = (imageio.imread(data[i]))\n",
    "        test_pet[i, :, :] = 0.2989 * train_other[i, :, :, 0] + 0.5870 * train_other[i, :, :, 1] + 0.1140 * train_other[i, :, :, 2]\n",
    "\n",
    "        test_pet[i, :, :] = (test_pet[i, :, :] - np.min(test_pet[i, :, :])) / (\n",
    "                    np.max(test_pet[i, :, :]) - np.min(test_pet[i, :, :]))\n",
    "        test_pet[i, :, :] = np.float32(test_pet[i, :, :])\n",
    "    # expand dimension to add the channel\n",
    "    test_pet = np.expand_dims(test_pet, axis=1)\n",
    "    # verify the shape matches the pytorch standard\n",
    "    print(test_pet.shape)\n",
    "\n",
    "    # convert the PET Testing data to pytorch tensor\n",
    "    test_pet_tensor = torch.from_numpy(test_pet).float()\n",
    "    test_pet_tensor = test_pet_tensor.to(device)\n",
    "    print(test_pet_tensor.shape)\n",
    "    test_pet_tensor.requires_grad = True\n",
    "\n",
    "    return test_pet_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepTreeFuse(\n",
       "  (one_lf1): _LFConv1(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (one_lf2): _LFConv2(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(2, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(4, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): Conv2d(8, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(16, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (one_hf1): _HFConv1(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (one_hf2): _HFConv2(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (two_lf1): _LFConv1(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (two_lf2): _LFConv2(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 2, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(2, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(4, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): Conv2d(8, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(16, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (two_hf1): _HFConv1(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (two_hf2): _HFConv2(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv6): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (l_o): forOriginal(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (h_o): forOriginal(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (lf_recons): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (hf_recons): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "  )\n",
       "  (recon): Sequential(\n",
       "    (0): Conv2d(16, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (hf2con): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (reconse): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (re): Sequential(\n",
       "    (0): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (downsample): downsample(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (dsconv1): DsConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (globalfeature): SeFeature(\n",
       "    (block): Sequential(\n",
       "      (0): SegConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SegConv2(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pool_up): PoolandUp(\n",
       "      (conv1): SegConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (out): SegConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (otherconv): SegConv2(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (merge): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (segclass): segment(\n",
       "    (conv1): DsConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (dropandplt): Sequential(\n",
       "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (x0): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (x1): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (x2): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (xxx): Sequential(\n",
       "    (0): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (aff): AffineTransform()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "dtn = net()\n",
    "dtn.load_state_dict(torch.load('./MineCheckpoint.pth')) \n",
    "dtn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 256, 256)\n",
      "torch.Size([2, 1, 256, 256])\n",
      "(2, 1, 256, 256)\n",
      "torch.Size([2, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [-0.14715349674224854, 0.7918640375137329]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [-0.14714649319648743, 0.7920047640800476]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# predicted the fused image\n",
    "test_mri_tensor = load_mri_to_do_test()\n",
    "test_pet_tensor = load_pet_to_do_test()\n",
    "\n",
    "fused = dtn(test_mri_tensor.to(device),test_pet_tensor.to(device))\n",
    "fused_numpy = fused.data.cpu().numpy()\n",
    "print(test_mri_tensor.shape[0])\n",
    "\n",
    "for i in range(test_mri_tensor.shape[0]):\n",
    "    out_pic=fused_numpy[i,0,:,:]\n",
    "    name=str(i)\n",
    "    imageio.imwrite('./fusion_results/im_'+name+'.png', out_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
